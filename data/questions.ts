export const SAMPLE_MARKDOWN = `
#### Dato il seguente tableau di un problema PLC in forma minimizzazione: 

$$
\\begin{array}{|c|ccccc|}
\\hline
x_1 & x_2 & x_3 & x_4 & x_5 \\\\
\\hline
0 & 0 & 0 & \\alpha & \\beta & -3 \\\\
1 & 0 & 2 & 0 & \\gamma & 2 \\\\
0 & -1 & 4 & 1 & -2 & \\delta \\\\
\\hline
\\end{array}
$$

1. se $\\alpha \\ge 0$, $\\beta \\ge 0$ e $\\delta < 0$, le colonne di $x_1$ e $x_2$ danno una soluzione duale ammissibile.
2. se  $\\alpha \\ge 0$, $\\beta \\ge 0$ e $\\delta > 0$, le colonne di $x_1$ e $x_2$ danno una soluzione duale ammissibile  
3. se $\\alpha = 0$, $\\beta > 0$ e $\\delta \\ge 0$, il tableau √® associato alla soluzione primale ottima
4. se $\\alpha < 0$, $\\beta < 0$, $\\delta > 0$ e $\\gamma \\le 0$, il problema √® illimitato  
5. se $\\alpha = 0$, $\\delta > 0$ e $\\gamma > 0$, il problema primale non √® ammissibile 
6. se $\\alpha = 0$, $\\delta > 0$, il problema primale non √® vuoto
7. se $\\alpha = 0$, $\\beta \\ge 0$ danno una soluzione ottimale 
8. se $\\alpha = 0$, $\\beta \\ge 0$ , le colonne di $x_1$ e $x_2$ danno una soluzione duale ammissibile  

> Risposte: 
> Per le prime due domande 1 e 2, guardiamo un tableu di esempio:Es:
> $$
> \\begin{array}{|c|ccccc|}
> \\hline
> x_1 & x_2 & x_3 & x_4 & x_5 \\\\
> \\hline
> 0 & 0 & 0 & 5 & 5 & -3 \\\\
> 1 & 0 & 2 & 0 & -5 & 2 \\\\
> 0 & -1 & 4 & 1 & -2 & \\delta \\\\
> \\hline
> \\end{array}
> $$
> 1.‚ùå Le colonne di $x_1$ e $x_2$ danno una soluzione duale ammissibile?
>
> $x_1=(0,1,0)^T$  ‚áí √® gi√† una colonna ‚Äúdi base‚Äù.
>
> $x_2 = (0,0,-1)^T$ ‚áí √® una colonna di base **a segno negativo** , quindi non e' in forma canonica(basta moltiplicare la 3¬™ riga per $-1$ per ottenere il pivot $+1$).
>
> Quindi la base naturale √® $\\{x_1,x_2\\}$. La **dual ammissibilit√†**, letta dal tableau, dipende dal **segno dei costi ridotti** dei non base:
> $$
> \\bar c_3 = 0,\\quad \\bar c_4 = 5,\\quad \\bar c_5 = 5.
> $$
> Se stai risolvendo un **problema di minimizzazione**, la dual ammissibilit√† richiede $\\bar c_j \\ge 0$ per tutti i non base ‚áí **S√å**, √® dualmente ammissibile (e questo non dipende da $\\delta$).
> Se invece fosse stato un **problema di massimizzazione**, tipicamente serve $\\bar c_j \\le 0$ ‚áí qui **NO** (perch√© $5,5>0$).
>
> 2.‚ùå Il tableau √® associato alla soluzione primale ottima?
> Per essere ‚Äúprimale ottima‚Äù  servono **ammissibilit√† primale** + **costi ridotti col segno giusto**(ok, 0,5,5 ‚â• 0). Per la **fattibilit√†** primale i termini noti devono essere positivi ma non possiamo perche' non sappiamo se il $\\delta$ sia positiva o negativa.
>
> 3.‚úÖEs:
> $$
> \\begin{array}{|c|ccccc|}
> \\hline
> x_1 & x_2 & x_3 & x_4 & x_5 \\\\
> \\hline
> 0 & 0 & 0 & 0 & 5 & -3 \\\\
> 1 & 0 & 2 & 0 & \\gamma & 2 \\\\
> 0 & -1 & 4 & 1 & -2 & 5 \\\\
> \\hline
> \\end{array}
> $$
> il tableau √® associato alla soluzione primale ottima? Si perch√© la base $x_2$ e $x_4$ √® ammissibile e i costi ridotti sono tutti non negativi.
>
> 4.‚úÖEs:
> $$
> \\begin{array}{|c|ccccc|}
> \\hline
> x_1 & x_2 & x_3 & x_4 & x_5 \\\\
> \\hline
> 0 & 0 & 0 & -5 & -5 & -3 \\\\
> 1 & 0 & 2 & 0 & -5 & 2 \\\\
> 0 & -1 & 4 & 1 & -2 & 5 \\\\
> \\hline
> \\end{array}
> $$
> Dalle righe dei vincoli si vede che le colonne di $x_1$ e $x_4$ formano la matrice identit√†, quindi la base √® $\\{x_1, x_4\\}$ e la soluzione di base associata √®: $x_1 = 2,\\quad x_4 = 5,\\quad x_2=x_3=x_5=0,$
>
> che √® **primale ammissibile** (termini noti non negativi).
>
> Per un **problema di minimo**, una variabile non basica con **costo ridotto negativo** √® una candidata entrante perch√© pu√≤ **ridurre** il valore dell‚Äôobiettivo. Tra le non basiche $(x_2,x_3,x_5)$, l‚Äôunica con valore negativo in riga 0 √® $x_5$ (coefficiente $-5$), quindi scegliamo $x_5$ come **variabile entrante**.
>
> Ora osserviamo la colonna di $x_5$ nelle righe dei vincoli: i coefficienti sono $-5$ e $-2$, quindi sono **tutti** $\\le 0$. Questo implica che, aumentando $x_5$, i valori delle variabili basiche **non diminuiscono** (anzi aumentano), quindi la fattibilit√† primale **non viene mai violata**. Di conseguenza **non esiste una variabile uscente** (il ratio test non si pu√≤ applicare).
>
> Quindi possiamo aumentare $x_5$ arbitrariamente e l‚Äôobiettivo continua a diminuire senza limite: il problema √® **illimitato inferiormente**.
>
> 5.‚ùåEs:
> $$
> \\begin{array}{|c|ccccc|}
> \\hline
> x_1 & x_2 & x_3 & x_4 & x_5 \\\\
> \\hline
> 0 & 0 & 0 & 0 & \\beta & -3 \\\\
> 1 & 0 & 2 & 0 & 5 & 2 \\\\
> 0 & -1 & 4 & 1 & -2 & 5 \\\\
> \\hline
> \\end{array}
> $$
> I termini noti sono positivi, quindi il primale √® ammissibile.
>
> 6.‚úÖ
> $$
> \\begin{array}{|c|ccccc|}
> \\hline
> x_1 & x_2 & x_3 & x_4 & x_5 \\\\
> \\hline
> 0 & 0 & 0 & 5 & \\beta & -3 \\\\
> 1 & 0 & 2 & 0 & \\gamma & 2 \\\\
> 0 & -1 & 4 & 1 & -2 & 5 \\\\
> \\hline
> \\end{array}
> $$
> Esiste ed √® primale una soluzione di base ammissibile $x*= {2,0,0,5,0}$, l‚Äôinsieme fattibile non √® vuoto, cio√® significa che esiste **almeno un punto**, una soluzione che soddisfa **tutti i vincoli del problema**.
>
> Esempio per 7 e 8:
> $$\\begin{array}{|c|ccccc|}
> \\hline
> x_1 & x_2 & x_3 & x_4 & x_5 \\\\
> \\hline
> 0 & 0 & 0 & 0  & 5 & -3 \\\\
> 1 & 0 & 2 & 0 & \\gamma & 2 \\\\
> 0 & 1 & -4 & -1 & 2 & -\\delta \\\\
> \\hline
> \\end{array}$$
> 7.‚úÖ Per un problema di minimizzazione se una soluzione di base √® **primale ammissibile** e **tutti i costi ridotti delle variabili non basiche sono non negativi**, allora la soluzione √® **ottima**.
> 8.‚úÖ. La base del tableau √® $\\{x_1,x_4\\}$. Per ottenere $\\{x_1,x_2\\}$ si moltiplica per $-1$ la seconda riga, cos√¨ la colonna di $x_2$ diventa unitaria. Nel caso di **minimizzazione** la **dual ammissibilit√†** vale se $\\bar c_j \\ge 0 \\ \\ \\forall j,$  e dipende **solo** dai **costi ridotti** (riga 0: $(0,0,0,0,5)$), **non** dai termini noti (RHS).
---
#### Considera i due problemi,  $P: \\min \\{\\, c^T x : A x = b,\\, x \\ge 0 \\,\\}$ e $D: \\max \\{\\, u^T b : u^T A \\le c^T \\,\\}$ (dove $u$ √® libera in segno, perch√© nel primale c‚Äô√® un vincolo di uguaglianza). allora:

1. una soluzione ammissibile di $P$ fornisce un limite inferiore per il valore ottimale di $D$;  

2. una soluzione ammissibile di $D$ fornisce un limite inferiore per il valore ottimale di $P$;  

3. una coppia di soluzioni $x$ e $u$ con $x$ ammissibile per $P$ e $u$ ammissibile per $D$ sono ottimali se $c^T - c_B^TB^{-1}A \\geq 0$;  

4. una coppia di soluzioni $x$ e $u$ tale che $(u^TA - c^T)x = 0$ sono ottimali se $x \\geq 0$;  

5. una coppia di soluzioni $x$ e $u$ sono ottimali se $x \\geq 0$ e $u \\geq 0$.
> Risposte:
> 1. ‚ùå. Per debole dualit√† ($c^\top x \ge b^\top u$): ($c^\top x$) √® un upper bound di $ D^* $), non un lower bound.
> 2. ‚úÖ. Sempre da debole dualit√†: ($b^\top u \le c^\top x$) per ogni (x) ammissibile ‚áí ($b^\top u \le$ $P^*$) ‚áí lower bound per $P^*$
> 3. ‚ùå La condizione $c^\top - c_B^\top B^{-1}A \ge 0$
>    controlla **solo il duale** e non dice nulla su (x). Essa indica semplicemente che i ‚Äúprezzi‚Äù (u) sono **ammissibili per il problema duale**, perch√© i **costi ridotti sono non negativi** rispetto a una base (B) (dual-ammissibilit√†). Tuttavia, **non verifica se le quantit√† (x) sono fattibili**, cio√® se (x) rispetta i vincoli del primale; in particolare, non controlla la **complementarit√† (slackness)**. Per trasformarla in un‚Äôaffermazione vera: una coppia ((x,u)) √® ottimale se (equivalentemente) vale la **complementarit√†** $
>    (c^\top - u^\top A)x = 0,$ oppure se la condizione $c^\top - c_B^\top B^{-1}A \ge 0$
>    vale **insieme** a $x_B = B^{-1}b \ge 0,\qquad x_N = 0, $cio√® se (x) √® la **soluzione di base associata alla base (B)** ed √® primale ammissibile.
> 4. ‚ùåLa condizione $(u^\top A - c^\top)x = 0 $dice solo che **ogni variabile positiva ha costo ridotto nullo** (condizione di **complementarit√†**), ma **non garantisce** che: (x) soddisfi $Ax = b$; , (u) rispetti $u^\top A \le c^\top$. Per rendere l‚Äôaffermazione vera, bisogna dire che una coppia ((x,u)) √® **ottimale** se: $Ax=b,\quad x\ge 0,\quad u^\top A\le c^\top,\quad (c^\top-u^\top A)x=0. $(Cio√®: **ammissibilit√† primale + ammissibilit√† duale + complementarit√†**.
> 5. ‚ùå La sola non negativit√† di (x) e (u) **non implica l‚Äôottimalit√†**; inoltre (u) **pu√≤ anche non essere positivo**, poich√© nel duale √® libero in segno.Per verificare l‚Äôottimalit√† √® necessario controllare: 1. i **vincoli del primale**; 2. i **vincoli del duale**; 3. la **complementarit√†** tra (x) e (u).
>
> üîë Riassumendo ‚Äì Regola d‚Äôoro
>
> Una coppia ((x,u)) √® **ottimale** se e solo se:
>
> 1. (x) √® ammissibile
>
> - rispetta i vincoli (Ax=b);
> - √® non negativo.
>
> 2. (u) √® ammissibile
>
> - non viola i vincoli duali (u^\top A \le c^\top).
>
> 3. Sono coerenti tra loro
>
> - nessuna variabile positiva ‚Äúspreca valore‚Äù
>   (condizione di **complementarit√†**).
>
> üëâ **Servono tutte e tre insieme**, mai una sola.
---
#### Una soluzione di base di un problema di PLC e' chiamata degenere quando: 

1. ci sono piu' variabili rispetto ai vincoli;

2. il numero di variabili di base con valore zero e' uguale al numero di vincoli;

3. il numero di variabili di base con valore diversi da zeri √® inferiore al numero di vincoli;

4. il numero di variabili di base con valore diversi da zeri √® uguale al  numero di righe dei vincoli della matrice;

5. il numero di variabili di base con valore zero √® uguale alla differenza di numero di colonne e righe (n - m);

6. il numero delle variabili non-nulle in base √® strettamente minore al numero di righe della matrice dei vincoli;

7. la matrice di base e' invertibile.

8. vi sono variabili nulle nella soluzione;

> Risposte:
> ### Definizione corretta di degenerazione (BFS,Basic Feasible Solution)
>
> In un problema di **Programmazione Lineare** (LP/PLC), una **soluzione di base** √® **degenere** quando **almeno una variabile di base vale 0**
> (equivalentemente: il numero di variabili di base **strettamente positive** √® **< m**, dove *m* √® il numero di vincoli / righe della matrice dei vincoli).
>
> Quindi: *‚Äúci sono variabili nulle nella soluzione‚Äù* **non √® sufficiente** in generale, perch√© in una BFS ci saranno sempre molte variabili **non di base** poste a 0. La degenerazione riguarda **le variabili di base**.
>
> Esempio di tableu degenere:
>
> $\begin{array}{|c|ccccc|}
> \hline
> x_1 & x_2 & x_3 & x_4 & x_5 \\
> \hline
> 0 & 0 & 0 & 5 & 3 & -3 \\
> 1 & 0 & 2 & 0 & 6 & 2 \\
> 0 & 1 & 4 & 1 & -2 & 0 \\
> \hline
> \end{array}$
>
> Quindi la soluzione fattibile di base(BFS)  associata √® $x^*=(2,0,0,0,0)$ con $x_3=x_4=x_5=0$ non basiche.
>
> La soluzione √® **degenere** perch√© **una variabile di base √® nulla**: $x_2$ √® in base e vale (0). Valutazione delle risposte:
>
> ‚úÖ Corrette:
> - **8.** ‚Äúvi sono variabili nulle nella soluzione‚Äù ‚Üí corretta **solo se intesa come: variabili di base nulle** Attenti, da chiedere al prof.
> - **3.** "n variabili di base ‚â†0 < n vincoli‚Äù ‚Üí √® proprio l‚Äôequivalente della degenerazione.
> - **6.** √® la stessa idea della 3, solo riscritta.
>
> ‚ùå Errate / non definitorie:
> - **1.** irrilevante per degenerazione.
> - **2.** troppo forte: ‚Äún basiche a zero = n vincoli‚Äù significherebbe tutte le basiche zero (caso particolare, non definizione).
> - **4.** descrive il caso **non degenere** (basiche non nulle = m).
> - **5.** non √® collegata alla definizione (mix n‚àím).
> - **7.** una base deve essere invertibile per definizione di base (non caratterizza degenerazione).
---

#### Dato un problema di Programmazione Lineare Intera (PLI), in minimizzazione, sia ( $z^*$ ) il valore ottimale del rilassamento continuo (LP-relax).

1. ( $z^*$ ) √® un limite superiore al valore IP ottimale
2. ( $z^*$ ) √® un limite inferiore al valore IP ottimale
3. ( $\\lceil z^* \\rceil$ ) (estremo superiore di ( $z^*$ )) √® un limite inferiore al valore IP ottimale se i coefficienti della funzione obiettivo sono interi
4. ( $\\lceil z^* \\rceil$ ) √® un limite superiore al valore IP ottimale
5. ( $\\lceil z^* \\rceil$ ) √® un limite inferiore al valore IP ottimale se i coefficienti della funzione obiettivo sono razionali

> #### Risposte:
>
> 1. ‚úÖ ‚Äî In minimizzazione, LP-relax ‚â§ IP ‚áí il valore LP √® upper bound sull‚Äôottimo intero.
> 2. ‚ùå ‚Äî √à il contrario: non √® un limite inferiore ma superiore.
> 3. ‚úÖ ‚Äî Con coeff. interi, $( \\lceil z^* \\rceil )$ produce un lower bound valido per l‚ÄôIP.
> 4. ‚ùå ‚Äî L‚Äôestremo superiore (ceiling) fornisce un lower bound, non upper bound.
> 5. ‚ùå ‚Äî Il risultato richiede coeff. interi, non solo razionali.

---

#### La strategia iniziale di esplorazione approfondita per il metodo branch-and-bound:

1. esplora prima il problema con il limite inferiore pi√π piccolo;

2. esplora prima il problema con il limite inferiore pi√π alto;

3. partendo da un problema padre, esplora il primo problema figlio generato;

4. partendo da un problema padre, esplora tutti i problemi figlio generati prima di considerare i problemi di livello inferiore;

5. dopo una fase di backtracking, esplora il primo problema figlio non esplorato, se presente.

6. viene utilizzato per trovare il percorso pi√π breve in una rete;

7. utilizza il rilassamento continuo per calcolare un limite in tutti i nodi;

8. esplora sempre 2n problemi;

9. esplora un numero di problemi non superiore al numero di variabili;

10. viene utilizzato per trovare la complessita' computazionale di problema;

11. puo' essere applicato a problemi di minimizzazione in forma standard;

12. trova l'ottimo soluzione di un problema NP-completo;

13. utilizzare sempre come procedura di delimitazione la soluzione di a Problema PLC;

> #### Risposte:
>
> 1. ‚ùå ‚Äî Selezionare il nodo col limite inferiore pi√π piccolo √® una strategia best-bound, non la deep-first iniziale.
>
> 2. ‚ùå ‚Äî Idem: scegliere il limite inferiore pi√π alto non √® deep-first e sarebbe comunque una scelta poco promettente nel caso di minimizzazione.
>
> 3. ‚úÖ ‚Äî In depth-first, da un padre si scende sul primo figlio generato e si continua ad approfondire la stessa ramificazione.
>
> 4. ‚ùå ‚Äî Esplorare tutti i figli prima di scendere di livello descrive la breadth-first, non la deep-first.
>
> 5. ‚úÖ ‚Äî Dopo il backtracking, la deep-first prosegue col primo figlio non ancora esplorato del nodo corrente.
>
> 6. ‚ùå ‚Äî Per il percorso minimo si usano algoritmi polinomiali (Dijkstra, Bellman-Ford); B&B non √® lo strumento tipico.
>
> 7. ‚úÖ ‚Äî Nei MILP si usa normalmente il rilassamento continuo (LP) per calcolare il bound in ciascun nodo visitato (per potare).
>
> 8. ‚ùå ‚Äî Non ‚Äúsempre 2n‚Äù: il numero di nodi pu√≤ essere esponenziale (‚âà2^n) nel peggiore dei casi, o molto meno se si pota bene.
>
> 9. ‚ùå ‚Äî Non √® limitato al numero di variabili; pu√≤ esplorare molti pi√π nodi.
>
> 10. ‚ùå ‚Äî B&B non si usa per determinare la complessit√† computazionale di un problema (questo √® tema di teoria delle complessit√†).
>
> 11. ‚úÖ ‚Äî Si pu√≤ applicare a problemi di minimizzazione (in qualsiasi forma standard equivalente).
>
> (n) ‚úÖ ‚Äî √à un metodo esatto: pu√≤ trovare l‚Äôottimo anche per problemi NP-completi (senza per√≤ garantire tempi polinomiali).
>
> 13. ‚ùå ‚Äî Non ‚Äúsempre‚Äù LP: il bound pu√≤ venire anche da euristiche, Lagrangiano, surrogate ecc.; l‚ÄôLP √® comune ma non obbligatorio.

---

#### Il metodo branch-and-bound per problemi di zaino 0-1:

1. utilizza un albero decisionale con un numero esponenziale di nodi

2. utilizza la strategia piu' bassa per esplorare l'albero decisionale

3. calcola un limite superiore in ogni nodo dell'albero decisionale

4. utilizza un albero decisionale con un numero polinomiale di livelli

5. utilizza un albero decisionale con un numero esponenziale di livelli

6. Ha $O(2^n)$ livelli degli alberi

7. Calcola il limite superiore in tutti i nodi generati da un vincolo del genere $x_j$=1

8. Usa la prima strategia pi√π alta per esplorare l‚Äôalbero

9. Calcola il limite superiore solo nei nodi figlio generati aggiungendo un vincolo del genere $x_j$=0, al nodo padre

10. Interrompe la ricerca non appena il livello n √ã raggiunto

> #### Risposte:
>
> 1. ‚úÖ ‚Äî L‚Äôalbero pu√≤ avere numero di nodi esponenziale (fino a ‚âà2^n foglie) nel caso peggiore; la potatura pu√≤ solo ridurli.
>
> 2. ‚ùå ‚Äî Per un problema di massimizzazione non si esplora il nodo col bound pi√π basso; tipicamente si usa best-bound col bound pi√π alto oppure depth-first.
>
> 3. ‚úÖ ‚Äî A ogni nodo (vivo) si calcola un limite superiore (es. zaino frazionario/LP) per poter potare.
>
> 4. ‚úÖ ‚Äî L‚Äôalbero ha n livelli (uno per item), quindi numero di livelli polinomiale (O(n)).
>
> 5. ‚ùå ‚Äî I livelli non sono esponenziali: sono n.
>
> 6. ‚ùå ‚Äî I livelli non sono O($2^n$); sono O(n). (Il numero di nodi pu√≤ essere esponenziale, non i livelli.)
>
> 7. ‚ùå ‚Äî Il bound si calcola su tutti i figli generati (sia con vincolo $x_j$=1 sia con $x_j$=0), non solo su quelli con $x_j$=1.
>
> 8. ‚úÖ ‚Äî Nella pratica per knapsack si usa spesso una strategia best-first scegliendo il nodo col bound pi√π alto (massimizzazione).
>
> 9. ‚ùå ‚Äî Non si calcola il bound solo sui figli con $x_j$=0; si calcola su entrambi i figli (se vivi).
>
> 10. ‚ùå ‚Äî Raggiungere il livello n in un ramo d√† una soluzione completa, ma non si interrompe finch√© tutti i nodi vivi non sono potati o valutati (serve garantire l‚Äôottimo globale).

---

#### Un problema NP-completo:

  1. non pu√≤ essere risolto esattamente in tempo polinomiale;

  2. pu√≤ essere risolto utilizzando l‚Äôalgoritmo del simplesso (primal o dual);

  3. ha sempre una complessit√† computazionale $(O(2^n))$;

  4. non pu√≤ essere risolto esattamente utilizzando computer all‚Äôavanguardia;

  5. pu√≤ essere risolto utilizzando il simplesso rivisto;

  6. pu√≤ essere risolto utilizzando il metodo branch-and-bound;

  7. pu√≤ essere risolto con una sequenza di calcoli di percorso pi√π breve;

  8. non pu√≤ essere risolto con un algoritmo esatto;

  9. pu√≤ essere risolto esattamente usando l‚Äôalgoritmo del simplesso con un numero di iterazioni non polinomiale;

  10. pu√≤ essere risolto esattamente usando l‚Äôalgoritmo del simplesso con l‚Äôaggiunta di tagli di Gomory (dopo opportuna formulazione come ILP);

  11. pu√≤ essere risolto esattamente usando l‚Äôalgoritmo di Dijkstra;

  12. pu√≤ essere risolto esattamente usando un albero decisionale binario con (n) livelli, dove (n) √® il numero di variabili;

  13. pu√≤ essere risolto con una sequenza di calcoli del percorso pi√π breve.

> #### Risposte:
>
> 1. ‚ùå ‚Äî Non √® dimostrato: sarebbe ‚úÖ solo se ($P\\neq NP$). Ad oggi non si sa se esista un algoritmo polinomiale.
> 2. ‚ùå ‚Äî Il simplesso risolve problemi LP; un NP-completo in generale non √® un LP.
> 3. ‚ùå ‚Äî Non ‚Äúsempre‚Äù ($O(2^n)$): esistono algoritmi esatti con altre complessit√† esponenziali o pseudos-esponenziali; la classe non impone un‚Äôunica forma.
> 4. ‚ùå ‚Äî Si pu√≤ risolvere esattamente con algoritmi esponenziali per istanze di taglia moderata; l‚Äôhardware non lo vieta.
> 5. ‚ùå ‚Äî Come (b): il simplesso rivisto resta un metodo LP, non risolve in generale problemi NP-completi.
> 6. ‚úÖ ‚Äî Branch-and-bound √® un metodo esatto (tipicamente esponenziale) applicabile dopo adeguata modellazione (es. ILP/0-1).
> 7. ‚ùå ‚Äî ‚ÄúPercorso pi√π breve‚Äù √® un problema in P; una sequenza di shortest path non risolve in generale NP-completi.
> 8. ‚ùå ‚Äî Esistono algoritmi esatti (ma non polinomiali noti in generale).
> 9. ‚ùå ‚Äî Anche con molte iterazioni, il simplesso resta per LP; non risolve in generale NP-completi.
> 10. ‚úÖ (con caveat) ‚Äî Molti NP-completi si formulano come ILP 0-1; tagli di Gomory (spesso con B&B/B&C) danno un metodo esatto in tempo potenzialmente esponenziale.
> 11. ‚ùå ‚Äî Dijkstra risolve shortest path con pesi non negativi (tempo polinomiale), non problemi NP-completi in generale.
> 12. ‚úÖ ‚Äî L‚Äôesaustiva su un albero binario di profondit√† (n) (backtracking) √® un algoritmo esatto (peggiore dei casi esponenziale).
> 13. ‚ùå ‚Äî √à la stessa idea di (g): shortest-path in sequenza non risolve, in generale, NP-completi.

---

#### Dato un problema PLC con n variabili e m vincoli, una matrice di base √®:

1. una raccolta di n vincoli;

2. una raccolta di n x m colonne della matrice dei vincoli;

3. una matrice quadrata m x m con valore 1 sulla diagonale principale;

4. una raccolta di m colonne linearmente indipendenti;

5. una sottomatrice quadrata n x n.

6. una sottomatrice quadrata della matrice dei vincoli, che pu√≤ essere invertita

7. una raccolta di m vincoli;

> #### Risposte:
>
> 1. ‚ùå ‚Äî Una matrice di base non √® una raccolta di vincoli, ma una raccolta di colonne della matrice dei vincoli che corrispondono alle variabili di base.
>
> 2. ‚ùå ‚Äî Una matrice di base non √® una raccolta di n x m colonne, ma solo n colonne della matrice dei vincoli, dove (n) √® il numero di variabili in base.
>
> 3. ‚ùå ‚Äî La matrice di base non √® una matrice quadrata con valori 1 sulla diagonale principale, ma una sottomatrice che pu√≤ essere invertibile (se le colonne corrispondenti sono linearmente indipendenti).
>
> 4. ‚úÖ ‚Äî Una matrice di base √® una raccolta di m colonne linearmente indipendenti dalla matrice dei vincoli. Queste colonne corrispondono alle variabili di base in una soluzione.
>
> 5. ‚ùå ‚Äî La matrice di base non √® una sottomatrice quadrata (n \\times n), ma una sottomatrice (m \\times m) che pu√≤ essere invertibile.
>
> 6. ‚úÖ ‚Äî Una matrice di base √® una sottomatrice quadrata della matrice dei vincoli che pu√≤ essere invertibile se le colonne scelte sono linearmente indipendenti.
>
> 7. ‚ùå ‚Äî Una matrice di base non √® una raccolta di m vincoli, ma una raccolta di colonne della matrice dei vincoli, corrispondenti alle variabili di base.

---

#### Il costo ridotto di una variabile di base ($x_j$), np un tableau in forma di base:

1. √® sempre maggiore o uguale a zero;

2. √® strettamente positivo se la soluzione corrente √® ottimale e il problema √® in forma di minimizzazione;

3. ha valore ($c^T B^{-1} A_j$);

4. √® uguale alla variazione della funzione obiettivo quando la variabile ($x_j$) aumenta di una unit√† (e tutte le altre variabili non di base rimangono al valore zero);

5. √® l'opposto della j-esima variabile duale;

6. √® sempre nullo;

7. ha un valore non positivo se la soluzione corrente √® ottimale e il problema √® in forma di minimizzazione;

8. rappresenta la variazione della funzione obiettivo quando la (rhs) del j-esimo vincolo aumenta di una unit√†;

9. ha un valore non negativo se la soluzione attuale √® ottimale e il problema √® in minimizzazione.

> #### Risposte:
>
> #### Per definizione i costi ridotti sono:  $c^T - c^TB^{-1}A_j$; e sono sempre ‚â• 0 se la soluzione √® ottima
>
> 1. ‚ùå ‚Äî Il costo ridotto di una variabile di base non √® sempre maggiore o uguale a zero. Per le variabili di base, il costo ridotto √® zero, mentre per le variabili non di base potrebbe essere negativo o positivo.
>
> 2. ‚ùå ‚Äî In un problema di minimizzazione, il costo ridotto di una variabile non di base deve essere minore di zero per indicare che il problema pu√≤ essere migliorato. Un costo ridotto positivo indica che la soluzione non √® ottimale.
>
> 3. ‚úÖ ‚Äî Il costo ridotto di una variabile di base ($x_j$) √® dato da ($c^T B^{-1} A_j$), dove 2. √® la matrice delle colonne di base e ($A_j$) √® la colonna del vincolo corrispondente alla variabile.
>
> 4. ‚úÖ ‚Äî Il costo ridotto rappresenta la variazione della funzione obiettivo quando la variabile ($x_j$) aumenta di una unit√†, mantenendo costanti le altre variabili non di base (che sono fissate a zero).
>
> 5. ‚ùå ‚Äî Il costo ridotto non √® l'opposto della variabile duale. La variabile duale di un vincolo rappresenta la sensibilit√† del valore della funzione obiettivo rispetto a un aumento del termine noto (rhs) del vincolo.
>
> 6. ‚ùå ‚Äî Il costo ridotto di una variabile di base non √® sempre nullo, ma √® nullo solo se la variabile √® sulla base e non cambia il valore della funzione obiettivo.
>
> 7. ‚úÖ ‚Äî Se la soluzione corrente √® ottimale e il problema √® di minimizzazione, il costo ridotto deve essere non positivo (zero o negativo) per tutte le variabili non di base. Un costo ridotto positivo implicherebbe una soluzione non ottimale.
>
> 8. ‚úÖ ‚Äî Il costo ridotto rappresenta la variazione della funzione obiettivo quando il termine noto (rhs) di un vincolo aumenta di una unit√†, con tutte le altre variabili non di base fissate.
>
> 9. ‚ùå ‚Äî Il costo ridotto non pu√≤ essere sempre non negativo. Se la soluzione √® ottimale e il problema √® di minimizzazione, il costo ridotto delle variabili non di base sar√† non positivo.

------

#### Dato un PLC di minimizzazione, il costo ridotto di una variabile non in base ($x_i$), in un tableau in forma di base:

1. √à strettamente positivo se la soluzione attuale √® ottimale;
2. √à la variazione della funzione obiettivo quando il termine noto del j-esimo vincolo aumenta di una unit√† (e tutte le variabili non di base rimangono uguali a zero);
3. √à la variazione della funzione obiettivo quando la variabile ($x_j$) cresce di un‚Äôunit√† (e tutte le variabili non di base rimangono uguali a zero);
4. √à sempre nullo;
5. Ha un valore non negativo se la soluzione corrente √® ottimale.

> #### Risposte:
>
> 1. ‚ùå ‚Äî Il costo ridotto non √® strettamente positivo se la soluzione √® ottimale; deve essere non positivo per tutte le variabili non di base. Se √® positivo, la soluzione non √® ottimale.
> 2. ‚úÖ ‚Äî Il costo ridotto di una variabile non di base rappresenta la variazione della funzione obiettivo quando il termine noto del j-esimo vincolo aumenta di una unit√†, con tutte le variabili non di base fissate.
> 3. ‚úÖ ‚Äî Il costo ridotto di una variabile non di base rappresenta la variazione della funzione obiettivo quando quella variabile cresce di una unit√†, con tutte le altre variabili non di base fissate a zero.
> 4. ‚ùå ‚Äî Il costo ridotto di una variabile non √® sempre nullo. In generale, √® diverso da zero e pu√≤ essere positivo o negativo, a seconda che la variabile non di base possa migliorare o meno la funzione obiettivo.
> 5. ‚úÖ ‚Äî Se la soluzione corrente √® ottimale e il problema √® di minimizzazione, il costo ridotto di tutte le variabili non di base sar√† non positivo.

---

#### Dato un problema PLC, l'analisi di sensibilit√† di un valore del lato destro ($b_i$) (anche chiamato right-hand side, rhs):

1. definisce un limite superiore per il valore della soluzione ottimale;

2. definisce il valore minimo e massimo della i-esima variabile che non modifica la base ottimale;

3. definisce il valore minimo e massimo di ($b_i$) che non modifica la base ottimale;

4. definisce l'intervallo di valori di ($b_i$) che non modifica i costi ridotti;

5. definisce l'intervallo di valori di ($b_i$) che non modifica le variabili duali;

6. definisce il valore minimo e massimo di 2. che non cambia il valore della soluzione ottima;

7. definisce l'intervallo di valori di 2. che non modifica le variabili duali;

> #### Risposte:
>
> 1. ‚ùå ‚Äî L'analisi di sensibilit√† di (b_i) non fornisce un limite superiore per la soluzione ottimale. Piuttosto, analizza come la soluzione ottimale cambia al variare del valore di ($b_i$), ma non impone un "limite superiore" per la soluzione ottimale stessa.
>
> 2. ‚ùå ‚Äî L'analisi di sensibilit√† non riguarda direttamente i valori minimi e massimi della variabile (x_i) che non cambiano la base ottimale, ma riguarda i vincoli del problema e come il cambiamento in ($b_i$) influenzi la base.
>
> 3. ‚úÖ ‚Äî L'analisi di sensibilit√† pu√≤ determinare l'intervallo di valori di ($b_i$) per i quali la base ottimale rimane invariata. Se il valore di ($b_i$) esce da questo intervallo, la base ottimale cambier√†.
>
> 4. ‚ùå ‚Äî L'intervallo di valori di ($b_i$) che non modifica i costi ridotti non √® parte dell'analisi di sensibilit√† in quanto i costi ridotti dipendono dalla variabile duale e non direttamente dal rhs ($b_i$).
>
> 5. ‚ùå ‚Äî L'intervallo di valori di (b_i) che non modifica le variabili duali non √® definito in questo modo. L'analisi di sensibilit√† pu√≤ determinare come i cambiamenti in ($b_i$) possano alterare il valore della soluzione ottimale e le variabili duali, ma non in un intervallo fisso.
>
> 6. ‚úÖ? ‚Äî L'analisi di sensibilit√† pu√≤ determinare l'intervallo di valori di ($b_i$) che non cambia il valore della soluzione ottimale. Questo intervallo stabilisce la stabilit√† della soluzione ottimale rispetto alle variazioni dei valori di ($b_i$).
>
> 7. ‚ùå ‚Äî L'analisi di sensibilit√† non definisce un intervallo di valori di 2. che non modifica le variabili duali. Le variabili duali cambiano in risposta alla modifica del rhs, ma l'analisi di sensibilit√† si concentra su come (b_i) influenzi la soluzione ottimale, non le variabili duali separatamente.

---

#### Dato un problema di flusso a costo minimo con sorgente (s), destinazione (t) e valore di flusso (q):

1. Il flusso √® ottimale se la quantit√† di flusso entrante in un qualsiasi vertice (j) √® uguale alla quantit√† di flusso uscente da (j);

2. In una soluzione ottima il flusso uscente da (s) √® uguale a (q);

3. Nel grafo residuo associato a una soluzione ottima non esiste alcun ciclo di costo negativo;

4. Nel grafo residuo associato a una soluzione ottima non esiste alcun ciclo di costo positivo;

5. Nel grafo residuo associato a una soluzione ottima non esiste alcun arco entrante in (t);

> #### Risposte:
>
> 1. ‚ùå ‚Äî La condizione che il flusso entrante in un nodo sia uguale al flusso uscente √® una condizione di conservazione del flusso in un grafo di flusso, ma non √® sufficiente a garantire l'ottimalit√† in termini di costo minimo. L'ottimalit√† √® determinata dal fatto che il flusso soddisfi anche il costo minimo.
>
> 2. ‚úÖ ‚Äî In una soluzione ottima di flusso, il flusso uscente dalla sorgente (s) deve essere esattamente uguale al valore di flusso (q), poich√© (q) rappresenta la quantit√† di flusso totale che deve essere inviato dalla sorgente alla destinazione.
>
> 3. ‚úÖ ‚Äî Un ciclo di costo negativo nel grafo residuo indica che esiste la possibilit√† di migliorare ulteriormente la soluzione (minimizzare il costo), quindi in una soluzione ottima, il grafo residuo non deve contenere cicli di costo negativo.
>
> 4. ‚ùå ‚Äî Il grafo residuo in una soluzione ottima pu√≤ contenere cicli di costo positivo, ma questi non portano a una riduzione del costo totale, poich√© il flusso ottimale √® gi√† stato trovato. I cicli di costo positivo non influiscono sull'ottimalit√† della soluzione.
>
> 5. ‚úÖ ‚Äî In una soluzione ottima, il flusso deve arrivare alla destinazione (t), quindi non esistono archi entranti in (t) nel grafo residuo. Gli archi entranti in (t) sono gi√† saturi, quindi non c'√® alcun flusso che pu√≤ entrare in (t) dalla destinazione.

---

#### Considera un grafo ( G = (V, E) ). Sia $S/V = \\{ x \\mid x \\in S \\ \\text{e} \\ x \\notin V \\}$ interpretato come tutti i nodi sorgente che NON sono anche nodi intermedi. 

Quale delle seguenti affermazioni descrive correttamente concetti relativi al problema del cammino pi√π breve da ($s$) a un vertice ($v$) (shortest path)?

1. Nel problema del cammino pi√π breve, un nodo sorgente appartenente a ( S/V ) pu√≤ essere usato come nodo intermedio se ci√≤ riduce la lunghezza del percorso.
2. Il problema del cammino pi√π breve da ( s ) a ( v ) richiede di considerare solo percorsi che non utilizzano nodi intermedi.
3. Nel problema del cammino pi√π breve, la soluzione ottimale consiste sempre in un albero di copertura minimo (Minimum Spanning Tree).
4. Un cammino pi√π breve da ( s ) a ( v ) pu√≤ includere nodi intermedi, ma un nodo sorgente appartenente a ( S/V ) non pu√≤ mai essere nodo intermedio.

> #### Risposte:
>
> 1. ‚ùå - Per definizione, un nodo in $S/V$ √® un nodo sorgente che non pu√≤ essere nodo intermedio.
>    Quindi NON pu√≤ essere utilizzato come nodo intermedio in un cammino, nemmeno se migliorerebbe il costo.
> 2. ‚ùå - Il cammino pi√π breve da $s$ a $v$ pu√≤ e di solito deve usare nodi intermedi.
>    Non √® ‚úÖ che si debbano considerare solo percorsi senza intermedi.
> 3. ‚ùå - Un albero di copertura minimo (MST) NON risolve il problema del cammino pi√π breve.
>    MST minimizza il costo totale dell‚Äôalbero, non il costo dei singoli cammini.
> 4. ‚úÖ - √à corretto:
>    - un cammino pi√π breve da $s$ a $v$ pu√≤ usare nodi intermedi;
>    - ma un nodo sorgente appartenente a $S/V$ √® definito come sorgente che non pu√≤ essere intermedio.

---

#### Sia $G=(V,E)$ un grafo non orientato con pesi (costi) sugli archi. Si consideri il problema dell‚Äôalbero di copertura di costo minimo (Minimum Spanning Tree, MST) e una sua soluzione ottima $T^*$.

Quali affermazioni sono corrette?

1. L‚Äôarco di costo minimo √® sempre in $T^*$.
2. L‚Äôarco di costo minimo √® in $T^*$ solo se non chiude un ciclo con altri archi.
3. L‚Äôarco di costo massimo √® sempre in $T^*$.
4. L‚Äôarco di secondo costo minimo √® sempre in $T^*$.
5. L‚Äôarco di secondo costo minimo √® in $T^*$ solo se l‚Äôarco di costo minimo non √® in $T^*$.

> #### Risposte:
>
> 1. ‚ùå ‚Äî Non √® ‚úÖ che un particolare arco di costo minimo debba appartenere a ogni MST.
>    Controesempio: triangolo con tre archi tutti di peso $1$. Ogni MST usa 2 archi qualunque, quindi un arco minimo specifico pu√≤ essere escluso.
> 2. ‚úÖ ‚Äî T √® un albero, quindi √® aciclico. Di conseguenza, ogni arco appartenente a $T^*$ non pu√≤ ‚Äúchiudere un ciclo‚Äù insieme agli altri archi di $T^*$.
>    Quindi l‚Äôimplicazione ‚Äúse l‚Äôarco minimo √® in $T^*$, allora non chiude un ciclo‚Äù √® sempre ‚úÖ (ed √® una condizione necessaria, non sufficiente).
> 3. ‚ùå ‚Äî Un arco di costo massimo pu√≤ benissimo essere evitato.
>    Controesempio: triangolo con pesi $1,2,100$. L‚ÄôMST usa gli archi $1$ e $2$, ed esclude $100$.
> 4. ‚ùå ‚Äî Neppure un arco di ‚Äúsecondo costo minimo‚Äù (inteso come arco specifico) √® garantito in $T^*$.
>    Controesempio: quadrilatero $A\\!-\\!B\\!-\\!C\\!-\\!D\\!-\\!A$ con pesi $AB=1$, $BC=2$, $CD=2$, $DA=2$. Un MST pu√≤ essere $\\{AB,CD,DA\\}$, che esclude l‚Äôarco $BC$ (anche se ha costo 2).
> 5. ‚ùå ‚Äî √à ‚ùå che il ‚Äúsecondo minimo‚Äù entri solo se il minimo non entra: spesso entrano entrambi.
>    Controesempio: cammino su 3 nodi $A\\!-\\!B\\!-\\!C$ con pesi $AB=1$, $BC=2$. L‚Äôunico MST √® $\\{AB,BC\\}$: contiene sia il minimo (1) sia il secondo minimo (2).

---

#### Si consideri il problema 0‚Äì1 Knapsack: $\\max \\left\\{\\sum_{j=1}^{n} p_j x_j \\;:\\; \\sum_{j=1}^{n} w_j x_j \\le c,\\; x_j\\in\\{0,1\\},\\; j=1,\\dots,n\\right\\}.$

Si consideri l‚Äôalgoritmo di programmazione dinamica che associa gli stati al profitto totale.

Quali affermazioni sono corrette?

1. Calcola un limite inferiore al valore ottimo.
2. Ha $c+1$ stati (da $0$ a $c$).
3. In ogni fase ha uno stato per ogni profitto $P$, con $P=0,\\dots,\\sum_{j=1}^n p_j$.
4. Alla fase $j$ si considera la possibilit√† di inserire l‚Äôoggetto $j$.
5. La soluzione ottima si trova nell‚Äôultimo stato dell‚Äôultima fase.

> #### Risposte:
>
> 1. ‚ùå - La DP (eseguita completamente) calcola il valore ottimo esatto, non solo un limite inferiore. Un limite inferiore √® tipico di euristiche o branch-and-bound, non della DP esatta.
>
> 2. ‚ùå - c+1 stati corrisponde alla DP con stati = capacit√†/peso (riempimento $0,\\dots,c$), non a quella con stati = profitto.
>
> 3. ‚úÖ - Se lo stato √® il profitto totale, allora lo spazio degli stati √® indicizzato da $P\\in\\{0,\\dots,\\sum p_j\\}$ (tipicamente si memorizza, per ogni profitto, il peso minimo necessario per ottenerlo).
>
> 4. ‚úÖ - In una DP a fasi per knapsack 0‚Äì1, alla fase $j$ si decide se includere o escludere l‚Äôoggetto $j$, aggiornando gli stati.
>
> 5. ‚ùå - Con stati = profitto, l‚Äôottimo non √® ‚Äúl‚Äôultimo profitto‚Äù $\\sum p_j$: in genere l‚Äôottimo √® il massimo profitto $P$ tale che il peso minimo associato a $P$ sia $\\le c$. Quindi non √® necessariamente l‚Äô‚Äúultimo stato‚Äù, ma il miglior stato ammissibile nell‚Äôultima fase.

---

#### Data una matrice $A$ di dimensione $m \\times n$, essa √® totalmente unimodulare se:

1. tutte le sottomatrici quadrate hanno determinante pari a $-1$, $0$ o $1$;

2. tutte le sottomatrici quadrate $m \\times m$ hanno determinante pari a $0$ o $1$;

3. il suo determinante √® definito;

4. il suo determinante ha valore $1$;

5. tutte le sottomatrici quadrate hanno determinante non nullo;

> #### Risposte:
>
> 1. ‚úÖ - Ogni sottomatrice quadrata ha determinante in $\\{-1, 0, 1\\}$. Questa √® la definizione formale di total unimodularity.
>
> 2. ‚ùå - Le sottomatrici considerate devono essere tutte quelle quadrate, non solo quelle $m \\times m$.
>    Inoltre devono poter essere anche $-1$, non solo $0$ o $1$.
>
> 3. ‚ùå - Il determinante di $A$ √® sempre definito solo per matrici quadrate, ma la total unimodularity riguarda tutte le sottomatrici quadrate, non solo il determinante di $A$ stessa.
>
> 4. ‚ùå - Pu√≤ anche avere determinante $0$ o $-1$.
>    Serve che tutte le sue sottomatrici quadrate rispettino $-1, 0, 1$, non solo quella principale.
>
> 5. ‚ùå - Molte sottomatrici di una matrice totalmente unimodulare hanno determinante zero (ad esempio, se hanno due righe uguali o colonne linearmente dipendenti).  Dire che devono essere ‚Äútutte non nulle‚Äù √® quindi errato.

---

#### Dato un problema PLC (P) in forma di minimizzazione, con 2 vincoli di tipo ‚Äú$\\ge$‚Äù e 1 vincolo di tipo ‚Äú=‚Äù e 5 variabili, considera il problema duale (D)

1. (D) ha 3 variabili e 5 vincoli;
2. (D) ha 5 variabili e 3 vincoli;
3. la prima variabile di (D) deve essere non negativa;
4. la terza variabile di (D) non ha restrizioni di segno;
5. la seconda variabile di (D) deve essere non positiva;
6. se il primale (P) ha una soluzione ottima finita, allora il duale (D) ha una soluzione ottima finita;
7. se il primale √® vuoto (inammissibile), il duale √® vuoto;
8. se il primale √® illimitato, allora il duale √® illimitato;
9. una soluzione duale ammissibile pu√≤ avere un valore inferiore a quello della soluzione primale ottima;
10. qualsiasi soluzione ammissibile del primale ha un valore maggiore o uguale a qualsiasi soluzione ammissibile del duale.

> #### Risposte:
>
> 1. ‚úÖ ‚Äî Duale: 3 vincoli ‚Üí 3 variabili, primale: 5 variabili ‚Üí 5 vincoli.
> 2. ‚ùå ‚Äî √à il contrario (3 variabili, 5 vincoli).
> 3. ‚úÖ ‚Äî Vincolo ‚Äú‚â•‚Äù nel primale ‚Üí variabile duale ‚â• 0.
> 4. ‚úÖ ‚Äî Vincolo ‚Äú=‚Äù ‚Üí variabile duale libera.
> 5. ‚ùå ‚Äî Anche il secondo vincolo √® ‚Äú‚â•‚Äù, quindi variabile duale ‚â• 0.
> 6. ‚úÖ ‚Äî Se il primale ha ottimo finito, anche il duale.
> 7. ‚ùå ‚Äî Primale vuoto ‚Üí duale pu√≤ essere vuoto o illimitato.
> 8. ‚ùå ‚Äî Primale illimitato ‚Üí duale inammissibile, non illimitato.
> 9. ‚úÖ ‚Äî Valore duale ‚â§ valore primale: pu√≤ essere pi√π basso.
> 10. ‚úÖ ‚Äî Propriet√† della dualit√† debole.
>
> | Primale ‚Üì / Duale ‚Üí | Soluzione finita | Vuoto (inammiss.) | Illimitato      |
> | ------------------- | ---------------- | ----------------- | --------------- |
> | Soluzione finita    | ‚úî                | ‚úó                 | ‚úó               |
> | Vuoto               | ‚úó                | non applicabile   | ‚úî               |
> | Illimitato          | ‚úó                | ‚úî                 | non applicabile |

---

#### Considera un problema di minimizzazione e l'algoritmo dual simplex:

1) ad ogni passo il valore della soluzione corrente e' un limite superiore al valore ottimale
2) ad ogni passo il valore della soluzione corrente e' un limite inferiore al valore ottimale
3) l'operazione di pivot viene eseguita sugli elementi  $\\alpha_{ij} > 0;$
4) l'operazione di pivot viene eseguita sugli elementi  $\\alpha_{ij} < 0;$
5) i costi ridotti $\\bar e^T$diventano non negativi nell'ultima iterazione (dando la soluzione ottimale)

> #### Risposte:
>
> 1. ‚ùå ‚Äì √® un limite inferiore (non superiore), con la precisazione che la soluzione primale corrente pu√≤ anche non essere ammissibile finch√© l‚Äôalgoritmo non termina.
> 2. ‚ùå ‚Äì Non √® un limite inferiore.
> 3. ‚ùå ‚Äì Il pivot non usa $a_{ij}>0$.
> 4. ‚úÖ ‚Äì Il pivot usa $a_{ij}<0$.
> 5. ‚úÖ ‚Äì All‚Äôultima iterazione i costi ridotti diventano non negativi.

---

##### Si consideri un problema di Programmazione Lineare (PL) in forma standard con $n$ variabili e $m$ vincoli, matrice dei vincoli $A \\in \\mathbb{R}^{m \\times n}$ (di rango $m$), una base $B$ (insieme di $m$ colonne linearmente indipendenti di $A$) e il tableau del simplesso associato a $B$.

Un‚Äôoperazione di pivot viene utilizzata per:

1. Trasformare, tramite pivot, la colonna della variabile entrante $j \\notin B$ in una colonna della matrice identit√† $I_m$ nel nuovo tableau (cio√® renderla una colonna unitaria), sostituendo una colonna della base.

2. Aggiungere una disuguaglianza valida al tableau corrente.

3. Trasformare il tableau corrente in un nuovo tableau associato a una base con cui condivide $n-1$ colonne.

4. Trasformare il tableau corrente in un nuovo tableau associato a una qualsiasi base.

5. Trasformare il tableau corrente in un nuovo tableau associato a una base con cui condivide $m-1$ colonne.

> #### Risposte:
>
> 1. ‚úÖ ‚Äì Un pivot rende basica la variabile entrante $j\\notin B$: nel nuovo tableau la sua colonna diventa una colonna unitaria di $I_m$ (e una colonna basica precedente esce).
> 2. ‚ùå ‚Äì Aggiungere una disuguaglianza valida √® tipico dei cutting planes, non del pivot del simplesso.
> 3. ‚ùå ‚Äì Un pivot cambia una sola colonna della base; una base ha $m$ colonne, quindi la condivisione corretta √® $m-1$, non $n-1$.
> 4. ‚ùå ‚Äì Un singolo pivot porta solo a una base adiacente (differisce per una colonna), non a ‚Äúuna qualsiasi‚Äù base.
> 5. ‚úÖ ‚Äì Dopo un pivot la nuova base condivide $m-1$ colonne con la base precedente.



---

#### Consideriamo un problema di minimizzazione e l'algoritmo primale del simplesso:

1. a ogni passo il valore della soluzione corrente √® un limite inferiore del valore ottimale;
2. a ogni passo il valore della soluzione corrente √® un limite superiore del valore ottimale;
3. l'operazione di pivot viene eseguita sugli elementi $\\alpha_{ij} < 0;$
4. l'operazione di pivot viene eseguita sugli elementi $\\alpha_{ij} > 0;$
5. i costi ridotti $\\bar c^T$ diventano non negativi nell'ultima iterazione (ottenendo la soluzione ottimale)

> #### Risposte:
>
> 1. ‚ùå ‚Äì In un problema di minimizzazione, qualunque soluzione primal ammissibile (e il simplesso primale mantiene proprio la primal-ammissibilit√†) ha valore di obiettivo maggiore o uguale all‚Äôottimo $z^*$. Quindi il valore corrente √® un limite superiore (upper bound), non un limite inferiore.
> 2. ‚ùå ‚Äì Non √® un limite superiore.
> 3. ‚úÖ ‚Äì Il pivot usa $\\alpha_{ij}<0$.
> 4. ‚ùå ‚Äì Non usa $\\alpha_{ij}>0$.
> 5. ‚úÖ ‚Äì All‚Äôultima iterazione i costi ridotti sono non negativi.

---

#### Il metodo del piano di taglio (cutting-plane method):

1. viene utilizzato per risolvere problemi di programmazione lineare con variabili continue;
2. termina con un numero polinomiale di iterazioni;
3. viene utilizzato per risolvere problemi NP-completi (‚Äúdifficili‚Äù);
4. a ogni iterazione utilizza il simplesso per risolvere il sottoproblema corrente;
5. utilizza la strategia ‚Äúpi√π bassa prima‚Äù per esplorare i sottoproblemi.

> #### Risposte:
>
> 1. ‚ùå ‚Äî I piani di taglio servono per programmazione intera, non per LP continui.
> 2. ‚ùå ‚Äî Il metodo non garantisce numero polinomiale di iterazioni.
> 3. ‚úÖ ‚Äî Si usa proprio per problemi NP-completi come i 0-1 ILP.
> 4. ‚úÖ ‚Äî Ogni iterazione risolve un LP con il simplesso (o altro solver LP).
> 5. ‚ùå ‚Äî Non usa strategie di esplorazione tipo branch-and-bound; non esiste ‚Äúpi√π bassa prima‚Äù.

------

#### Dato il modello di Programmazione Lineare Intera (PLI) $\\min {c^T x : x \\in P \\cap \\mathbb{Z}^n} \\quad \\text{con} \\quad P = {Ax = b,; x \\ge 0}$ e data la soluzione ottima (x^) del rilassamento continuo, il taglio di Gomory √®:

1. una disuguaglianza per (P) che soddisfa il lemma di Farkas;
2. una disuguaglianza ($\\alpha_F^T x_F \\ge \\alpha_0$) soddisfatta da qualsiasi ($x \\in P$);
3. una disuguaglianza ($\\alpha_F^T x_F \\ge \\alpha_0$) soddisfatta da qualsiasi ($x \\in P \\cap \\mathbb{Z}^n$), ma non da ($x^*$);
4. una disuguaglianza ($\\alpha_F^T x_F \\ge \\alpha_0$) soddisfatta da qualsiasi ($x \\in $P) ma non da ($x^*$);
5. nessuna delle precedenti.

> ####  Risposte:
>
> 1. ‚ùå ‚Äî Non √® definito tramite il lemma di Farkas come condizione primaria.
> 2. ‚ùå ‚Äî Il taglio non deve essere soddisfatto da tutti i punti di (P), ma solo da quelli interi.
> 3. ‚úÖ ‚Äî √à la definizione corretta del taglio valido: esclude (x^) ma √® valido per ogni punto intero ammissibile.
> 4. ‚ùå ‚Äî Deve escludere (x^), ma non deve essere valido per tutti i punti reali di (P).
> 5. ‚ùå ‚Äî Perch√© la risposta corretta √® la 3.

---

#### Dato un problema PLC (P) in forma standard con (n) variabili e m vincoli, si consideri il metodo in due fasi.  Il problema artificiale utilizzato nella prima fase:

1. ha m variabili e (n) vincoli;
2. ha al massimo (n + m) variabili e m vincoli;
3. ha una funzione obiettivo in forma di massimizzazione;
4. fornisce una base ammissibile per il problema (P) se la sua soluzione ottima ha valore zero;
5. fornisce una soluzione ammissibile e ottimale per il problema (P) se la sua soluzione ottima ha valore negativo.

> #### Risposte:
>
> 1. ‚ùå ‚Äî Il numero di vincoli rimane m; le variabili non diventano m, ma (n +) eventuali variabili artificiali.
> 2. ‚úÖ ‚Äî Le variabili possono arrivare fino a (n + m); i vincoli restano m.
> 3. ‚ùå ‚Äî In fase 1 la funzione obiettivo √® minimizzare la somma delle variabili artificiali.
> 4. ‚úÖ ‚Äî Se la soluzione ottima della fase 1 √® 0, si ottiene una base ammissibile per il problema originale.
> 5. ‚ùå ‚Äî Se il valore ottimo √® positivo, il problema originale √® inammissibile; non esiste il caso "valore negativo".

------

#### Si consideri l‚Äôalgoritmo di Dijkstra per il calcolo dei cammini minimi.

Quali delle seguenti affermazioni sono corrette?

1. Non √® applicabile a grafi aciclici.
2. Non √® utilizzabile in un grafo con archi a costo (peso) negativo.
3. Pu√≤ essere utilizzato per trovare un albero di copertura di costo minimo (Minimum Spanning Tree).
4. Pu√≤ essere utilizzato per trovare un cammino minimo in un grafo diretto, da una sorgente $s$ a una destinazione $t$.
5. Non √® utilizzabile in un grafo con cicli (circuiti) negativi.

> #### Risposte:
>
> 1. ‚ùå ‚Äî Dijkstra pu√≤ essere applicato a grafi aciclici: l‚Äôassenza di cicli non √® un problema; ci√≤ che conta √® l‚Äôassenza di pesi negativi.
> 2. ‚úÖ ‚Äî Dijkstra √® corretto solo se tutti i pesi degli archi sono $\\ge 0$. Con pesi negativi, la scelta ‚Äúgreedy‚Äù (fissare definitivamente la distanza minima quando un nodo viene estratto) pu√≤ diventare ‚ùå.
> 3. ‚ùå ‚Äî L‚Äôalbero di copertura minimo (MST) si calcola con algoritmi come Prim o Kruskal, non con Dijkstra: Dijkstra produce un albero dei cammini minimi da una sorgente (Shortest Path Tree), che in generale non coincide con un MST.
> 4. ‚úÖ ‚Äî Dijkstra trova i cammini minimi da sorgente singola in grafi diretti o non diretti, purch√© i pesi siano non negativi. In particolare, consente di ottenere il cammino minimo $s \\to t$ (eventualmente arrestando l‚Äôalgoritmo quando $t$ viene estratto).
> 5. ‚úÖ ‚Äî In presenza di cicli negativi raggiungibili, il ‚Äúcammino minimo‚Äù pu√≤ non essere ben definito (si pu√≤ ridurre il costo indefinitamente). Inoltre, un ciclo negativo implica la presenza di almeno un arco negativo, quindi le ipotesi di Dijkstra sono violate.

---

#### Un problema si dice di tipo polinomiale (classe $P$) se esiste un algoritmo che lo risolve in tempo (numero di operazioni) polinomiale nella dimensione dell‚Äôinput $n$.

Quali affermazioni sono corrette?

a) √à sempre risolvibile con il simplesso in un numero non polinomiale di iterazioni.

b) Deve essere risolto con un algoritmo di branch-and-bound.

c) √à risolvibile esattamente con l‚Äôalgoritmo di Dijkstra.

d) √à risolvibile con un algoritmo che esegue un numero polinomiale di operazioni.

e) Ammette anche un algoritmo con complessit√† al pi√π esponenziale, ad esempio $O(2^n)$.

> #### Risposte:
>
> 1. ‚ùå ‚Äî Il simplesso √® un algoritmo per programmazione lineare (LP), non un metodo ‚Äúuniversale‚Äù per qualunque problema in $P$. Inoltre non √® ‚úÖ che un problema polinomiale sia ‚Äúsempre‚Äù risolto col simplesso (n√© che servano ‚Äúsempre‚Äù iterazioni non polinomiali).
> 2. ‚ùå ‚Äî Il branch-and-bound √® tipico per problemi combinatori (spesso NP-hard) e non √® necessario per un problema in $P$: in $P$ esiste gi√† un algoritmo polinomiale.
> 3. ‚ùå ‚Äî Dijkstra risolve il cammino minimo a sorgente singola in grafi con pesi non negativi, non un generico problema polinomiale.
> 4. ‚úÖ ‚Äî Per definizione di problema ‚Äúpolinomiale‚Äù (classe $P$), esiste un algoritmo che lo risolve con un numero di operazioni $O(n^k)$ per qualche costante $k$.
> 5. ‚úÖ ‚Äî per un problema polinomiale, √® formalmente ‚úÖ ma banale, perch√© se esiste un algoritmo $O(n^k)$, allora lo stesso algoritmo √® anche $O(2^n)$ (un bound pi√π ‚Äúlargo‚Äù).

---
`;
